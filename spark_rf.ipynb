{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"]= os.environ[\"PATH\"] + 'C:\\\\Users\\\\Tata\\\\Downloads\\\\spark-2.2.0-bin-hadoop2.6\\\\spark-2.2.0-bin-hadoop2.6\\\\bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = 'pyspark-shell'\n",
    "os.environ[\"JAVA_HOME\"] = 'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_131'\n",
    "os.environ[\"HADOOP_HOME\"] = 'C:\\\\winutils'\n",
    "#os.environ[\"PYSPARK_PYTHON\"] = 'python3'\n",
    "\n",
    "os.environ[\"SCALA_HOME\"] = 'C:\\\\Program Files (x86)\\\\scala'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_HOME\"] = 'C:\\\\Users\\\\Tata\\\\Downloads\\\\spark-2.2.0-bin-hadoop2.6\\\\spark-2.2.0-bin-hadoop2.6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(os.environ['SPARK_HOME']+\"\\\\python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.environ['SPARK_HOME']+\"\\\\python\\\\lib\\\\py4j-0.10.4-src.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import py4j\n",
    "from pyspark import SparkContext, SparkConf, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionModel, LinearRegressionWithSGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionModel, LinearRegressionWithSGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tata\\Downloads\\spark-2.2.0-bin-hadoop2.6\\spark-2.2.0-bin-hadoop2.6\\python\\pyspark\\mllib\\regression.py:281: UserWarning: Deprecated in 2.0.0. Use ml.regression.LinearRegression.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use ml.regression.LinearRegression.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.928638123469\n"
     ]
    }
   ],
   "source": [
    "data=[\n",
    "    LabeledPoint(0.0,[0.0]),\n",
    "    LabeledPoint(1.0,[1.0]),\n",
    "    LabeledPoint(3.0,[2.0]),\n",
    "    LabeledPoint(2.0,[3.0])\n",
    "]\n",
    "lrm=LinearRegressionWithSGD.train(sc.parallelize(data),iterations=10,initialWeights=np.array([1.0]))\n",
    "print(lrm.predict(np.array([1.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlcontext.read.format('com.databricks.spark.csv').options(header='true').load('C:\\\\Users\\\\Tata\\\\Downloads\\\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: string, Survived: string, Pclass: string, Name: string, Sex: string, Age: string, SibSp: string, Parch: string, Ticket: string, Fare: string, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Embarked='Q'), Row(Embarked='C'), Row(Embarked='S'), Row(Embarked='')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import types\n",
    "\n",
    "def Embarked_transform(x):\n",
    "    if x != None:\n",
    "        return x\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "my_udf = udf(Embarked_transform, types.StringType())\n",
    "df = df.withColumn('Embarked', my_udf(df['Embarked']))\n",
    "df.select('Embarked').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "encoder = OneHotEncoder(inputCol=\"EmbarkedIndex\", outputCol=\"EmbarkedVec\")\n",
    "df_t2 = encoder.transform(indexed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: string, Survived: string, Pclass: string, Name: string, Sex: string, Age: string, SibSp: string, Parch: string, Ticket: string, Fare: string, Cabin: string, Embarked: string, EmbarkedIndex: double, EmbarkedVec: vector]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Title='Don'),\n",
       " Row(Title='Miss'),\n",
       " Row(Title='Col'),\n",
       " Row(Title='Rev'),\n",
       " Row(Title='Lady'),\n",
       " Row(Title='Master'),\n",
       " Row(Title='Mme'),\n",
       " Row(Title='Capt'),\n",
       " Row(Title='Mr'),\n",
       " Row(Title='Dr'),\n",
       " Row(Title='Mrs'),\n",
       " Row(Title='Sir'),\n",
       " Row(Title='Jonkheer'),\n",
       " Row(Title='Mlle'),\n",
       " Row(Title='Major'),\n",
       " Row(Title='Ms'),\n",
       " Row(Title='the Countess')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Get_Title(x):\n",
    "    y = x.split(\", \")[1]\n",
    "    z = y.split(\".\")[0]\n",
    "    return(z)\n",
    "\n",
    "my_udf2 = udf(Get_Title, types.StringType())\n",
    "df_t2 = df_t2.withColumn('Title', my_udf2(df_t2['Name']))\n",
    "df_t2.select('Title').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer2 = StringIndexer(inputCol=\"Title\", outputCol=\"TitleIndex\")\n",
    "model2 = stringIndexer2.fit(df_t2)\n",
    "indexed2 = model2.transform(df_t2)\n",
    "encoder2 = OneHotEncoder(inputCol=\"TitleIndex\", outputCol=\"TitleVec\")\n",
    "df_t = encoder2.transform(indexed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: string, Survived: string, Pclass: string, Name: string, Sex: string, Age: string, SibSp: string, Parch: string, Ticket: string, Fare: string, Cabin: string, Embarked: string, EmbarkedIndex: double, EmbarkedVec: vector, Title: string, TitleIndex: double, TitleVec: vector]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Family(x,y):\n",
    "    return(str(int(x)+int(y)))\n",
    "\n",
    "my_udf3 = udf(Family, types.StringType())\n",
    "df_t = df_t.withColumn('Family', my_udf3(df_t['SibSp'],df_t['Parch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: string, Survived: string, Pclass: string, Name: string, Sex: string, Age: string, SibSp: string, Parch: string, Ticket: string, Fare: string, Cabin: string, Embarked: string, EmbarkedIndex: double, EmbarkedVec: vector, Title: string, TitleIndex: double, TitleVec: vector, Family: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.233440860215055\n",
      "29.87763005780347\n",
      "25.14061971830986\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "print(df_t.filter(df_t['Pclass']==\"1\").agg(avg(col(\"Age\"))).collect()[0][0])\n",
    "print(df_t.filter(df_t['Pclass']==\"2\").agg(avg(col(\"Age\"))).collect()[0][0])\n",
    "print(df_t.filter(df_t['Pclass']==\"3\").agg(avg(col(\"Age\"))).collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_age(str_age, Pclass):\n",
    "    try:\n",
    "        return float(str_age)\n",
    "    except:\n",
    "        if(Pclass==\"1\"):\n",
    "            return (38.23)\n",
    "        if(Pclass==\"2\"):\n",
    "            return (29.87)\n",
    "        if(Pclass==\"3\"):\n",
    "            return (25.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IsCabin(str_cabin):\n",
    "    try:\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsFamily(str_family):\n",
    "    if(str_family!=\"0\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsFemFirst(str_sex, str_Pclass):\n",
    "    if(str_sex==\"female\" and str_Pclass == \"1\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsMaleNotFirst(str_sex, str_Pclass):\n",
    "    if(str_sex==\"male\" and (str_Pclass == \"2\" or str_Pclass==\"3\")):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transf(r):\n",
    "    return LabeledPoint(\n",
    "        int(r.Survived),\n",
    "        [\n",
    "            int(r.Pclass),\n",
    "            r.Sex == 'male',\n",
    "            float(r.Fare),\n",
    "            int(r.SibSp),\n",
    "            int(r.Parch),\n",
    "            int(r.Family), #new\n",
    "            IsFamily(r.Family), #new\n",
    "            parse_age(r.Age, r.Pclass), #new\n",
    "            IsCabin(r.Cabin), #new\n",
    "        ] + list(r.EmbarkedVec.toArray()) + list(r.TitleVec.toArray()) #new\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = df_t.rdd.map(transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[105] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[106] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "rfc = RandomForest.trainClassifier(train, numClasses=2,\n",
    "                             categoricalFeaturesInfo={},\n",
    "                            numTrees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "svm = SVMWithSGD.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "logReg = LogisticRegressionWithLBFGS.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes\n",
    "nb = NaiveBayes.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc(m, test):\n",
    "    values = test.map(lambda x: x.features)\n",
    "    yhat = m.predict(values)\n",
    "    y = test.map(lambda x: x.label)\n",
    "    comp = yhat.zip(y)\n",
    "    errors = comp.map(lambda x: abs(x[0]-x[1]))\n",
    "    return 1-errors.sum()/errors.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1score(m, test):\n",
    "    values = test.map(lambda x: x.features)\n",
    "    yhat = m.predict(values)\n",
    "    y = test.map(lambda x: x.label)\n",
    "    comp = yhat.zip(y)\n",
    "    precision1 = comp.map(lambda x:  x[0]*x[1])\n",
    "    precision2 = comp.map(lambda x: x[0]==x[1])\n",
    "    precision = precision1.sum()/precision2.sum()\n",
    "    recall1 = comp.map(lambda x:  x[0]*x[1])\n",
    "    recall2 = comp.map(lambda x: x[1]>0)\n",
    "    recall = recall1.sum()/recall2.sum()\n",
    "    f1score = 2*precision*recall/(precision+recall)\n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest\n",
      "accuracy:  0.850187265917603\n",
      "f1 score:  0.4833836858006043\n",
      "SVM\n",
      "accuracy:  0.6217228464419475\n",
      "f1 score:  0.05185185185185184\n",
      "Logistic Regression\n",
      "accuracy:  0.8426966292134832\n",
      "f1 score:  0.5106382978723404\n",
      "Naive Bayes\n",
      "accuracy:  0.722846441948\n",
      "f1 score:  0.350168350168\n"
     ]
    }
   ],
   "source": [
    "print(\"random forest\")\n",
    "print(\"accuracy: \", acc(rfc, test))\n",
    "print(\"f1 score: \", f1score(rfc,test))\n",
    "print(\"SVM\")\n",
    "print(\"accuracy: \", acc(svm, test))\n",
    "print(\"f1 score: \", f1score(svm,test))\n",
    "print(\"Logistic Regression\")\n",
    "print(\"accuracy: \", acc(logReg, test))\n",
    "print(\"f1 score: \", f1score(logReg,test))\n",
    "print(\"Naive Bayes\")\n",
    "print(\"accuracy: \", acc(nb, test))\n",
    "print(\"f1 score: \", f1score(nb,test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# добавить 5 новых фичей\n",
    "# 3 фичи высчитываются из имеющихся\n",
    "# хотя бы одна использует udf\n",
    "\n",
    "# попробовать 3 новых модели\n",
    "\n",
    "# f1 меру"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
